Материалы к конференции DUMP-Казать в 2019 году.

Исследование: Анализ логов с помощью баз данных

Исследовались логи публичной активности github.com за 2015-2018 годы с помощью нескольких баз данных:

* PostgreSQL с BRIN-индексами по времени
* PostgreSQL с партицйионированием по месяцам
* PostgreSQL-cstore_fdw - расширение PostgreSQL для колоночного хранения данных. https://github.com/citusdata/cstore_fdw
* ClickHouse - колоночная база данных https://clickhouse.yandex
* MemSQL - гибридная база данных (строковая in-memory, колоночная - на диске) https://www.memsql.com

Таблица с результатами моих замеров, в черновом виде: table.xlsx

Файлы для повторения эксперимента лежат в папке docker.

Все скрипты выполняются из папки docker

docker/docker-compose.yml - файл для запуска контейнеров через docker-compose.

Для запуска memsql нужно сначала зарегистрироваться на https://www.memsql.com для получения лицензионного ключа (бесплатно).
Ключ нужно положить в файл docker/memsql.env в виде LICENSE_KEY=...
Потом контейнер memsql нужно запустить два раза: первый раз база инициализируется, прочитает ключ, подготовится и контейнер завершит работу.
Начиная со второго раза будет запускаться уже как обычно (при удалении контейнера - его снова нужно запустить дважды).


Подготовка данных:
1. (опционально) скачать логи с http://www.gharchive.org в папку  datasource.
2. Сконвертировать логи в csv-формат: 

```
docker-compose run dataset /scripts/prepare-day.sh 2018-09-19 # Для конвертации одного дня
``` 

```
docker-compose run dataset /scripts/prepare-month.sh 2018-09 # Для конвертации одного месяца
```

Если на момент конвертации в папке datasource содержит файлы, то источником конвертации будет папка.
Иначе при конвертации логи будут качаться с http://www.gharchive.org

3. Для наливки данных в базу данных можно пользоваться командой вида:

```
./scripts/fill-db-container.sh postgres-brin 2015 ""
```

В качестве первого параметра задается фильтр для grep (применяется к именам файлов логов после конвертации) - чтобы иметь 
возможность импортировать данные частями.

Второй параметр перемещает папку с данными docker-контейнера в указанное место. Если параметр равен пустой строке "" - перемещения не происходит.
Это полезно чтобы сохранить результат импорта - для быстрого восстановления в случае порчи данных при экспериментах.

4. Для выполнения запроса в базу данных - можно делать их напрямую из контейнеров или воспользоваться командой вида
```
./scripts/check-query.sh elasticsearch "SELECT * FROM \"push*\" LIMIT 1"
```

Первым параметром идёт имя контейнера, вторым - запрос.

5. Для прогона всех запросов можно вызвать команду вида:

```
./scripts/check-container.sh postgres2
```

Где параметром выступает название контейнера с базой (запросы для elasticsearch не реализованы)

